{"metadata":{"kernelspec":{"display_name":"JavaScript","language":"javascript","name":"javascript"},"language_info":{"codemirror_mode":{"name":"javascript","version":6},"file_extension":".js","mimetype":"text/javascript","name":"javascript","nbconvert_exporter":"javascript"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"## Welcome to the micrograd visualization experiment\nThis notebook demonstrates a graph visualization of a small Multilayer Perceptron's loss function at the website [verticesandedges.net](https://verticesandedges.net). Each vertex is a term, and although labels are disabled for this simulation, they contain the gradient of each term, or in other words, the influence of the term on the final result. \n\nThe following code, the Value, Neuron, Layer and MLP come from @karpathy's [micrograd](https://github.com/karpathy/micrograd). The visualization I wrote based on a [2007 description]((https://arxiv.org/abs/0712.1549) of a force directed graph visualization by Canadian CS Professor Dr. Todd Veldhuizen.\n\nI translated the micrograd code from Python to Javascript, and substituted my own graph visualization for the graphviz calls. \n\nTo run this simulation, minimize the notebook (so you can see the middle of the page, and click Run All). Once the script added all the vertices, there'll be a swarm of edges. Give that a little time to settle (if they all disappear, start over, if you changed the Constants, refresh the page to reset them), then carefully increase first Repulsion, the Attraction by a factor of ten, by selecting the lower of each property, and sliding it one step to the right. The top slider is the base, the second the exponent. "},{"cell_type":"markdown","metadata":{},"source":"We use global variables to create classes that can be reused across cells."},{"cell_type":"code","execution_count":2,"metadata":{},"source":"Value = class Value {\n  constructor(data, children=[], op='', label=''){\n    this.id = `${Value.idPrefix}${++Value.idCount}`;\n    this.data = data;\n    this.prev = new Set(children);\n    this.op = op;\n    this.label = label;\n    this.grad = 0;\n    this._backward = () => {};\n  }\n\n  toString(){\n    return `Value ${this.label + ' '}(data=${this.data.toFixed(3)} grad=${this.grad.toFixed(3)})`;\n  }\n\n  add(other){\n    other = other instanceof Value ? other : new Value(other);\n    const out = new Value(this.data + other.data, [this, other], '+');\n    out._backward = () => {\n      this.grad += out.grad;\n      other.grad += out.grad;\n    };\n    \n    return out;\n  }\n\n  mul(other){\n    other = other instanceof Value ? other : new Value(other)\n    const out = new Value(this.data * other.data, [this, other], '*');\n    out._backward = () => {\n      this.grad += other.data * out.grad;\n      other.grad += this.data * out.grad;\n    }\n    \n    return out;\n  }\n\n  pow(other){\n    console.assert(`Argument other must be a Number.`, other instanceof Number);\n    const out = new Value(this.data**other, [this], `**${other}`);\n    out._backward = () => {\n      this.grad += (other * this.data**(other-1)) * out.grad;\n    }\n\n    return out;\n  }\n\n  relu(){\n    const out = new Value(this.data < 0 ? 0 : this.data, [this], 'ReLu');\n    out._backward = () => {\n      this.grad += (out.data > 0 ? 1 : 0) * out.grad;\n    }\n\n    return out;\n  }\n\n  backward(){\n    const topo = [];\n    const visited = new Set();\n    const buildTopo = (v) => {\n      if(!visited.has(v)){\n        visited.add(v);\n        for(let child of v.prev){\n          buildTopo(child);\n        }\n        topo.push(v);\n      }\n    };\n    buildTopo(this);\n\n    this.grad = 1.0;\n    for(let i = topo.length -1; i>=0; i--){\n      topo[i]._backward();\n    }\n    \n    return topo;\n  }\n\n  neg(){\n    return this.mul(-1);\n  }\n\n  sub(other){\n    other = other instanceof Value ? other : new Value(other);\n    return this.add(other.neg());\n  }\n\n  div(other){\n    return this.mul(other.pow(-1));\n  }\n\n  tanh(){\n    const x = this.data;\n    const t = (Math.exp(2*x) - 1)/(Math.exp(2*x) + 1);\n    const out = new Value(t, [this], 'tanh');\n    out._backward = () => {\n      this.grad += (1- t**2) * out.grad;\n    }\n\n    return out;\n  }\n\n  exp(){\n    const x = this.data;\n    const out = new Value(Math.exp(x), [this], 'exp');\n    out._backward = () => {\n      this.grad += out.data * out.grad;\n    }\n\n    return out;\n  }\n\n  static idPrefix = 'v'\n  static idCount = 0;\n}","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":"Neuron = class Neuron {\n  constructor(nin, nonlin=true){\n    this.w = Array(nin).fill(0).map(() => new Value((Math.random() * 2) - 1));\n    this.b = new Value(0);\n    this.nonlin = nonlin;\n  }\n\n  call(x){\n    const act = this.w\n      .filter((w, i) => i >= this.b.data)\n      .map((wi, i) => [wi, x[i]])\n      .reduce((acc, pair) => {\n        return acc.add(pair[0].mul(pair[1]))\n      }, new Value(0));\n    \n    const out = act.tanh();\n    return out;\n  }\n\n  parameters(){\n    return this.w.concat([this.b]);\n  }\n\n  toString(){\n    return `${this.nonlin ? 'ReLu' : 'Linear'} Neuron(${this.w.length})`;\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":0,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":"trace = (root) => {\n  let nodes = new Set([]);\n  let edges = new Set([]);\n\n  let build = (v) => {\n    if(!nodes.has(v)){\n      nodes.add(v);\n      for(let child of v.prev){\n        edges.add([child, v]);\n        build(child);\n      }\n    }\n  }\n  \n  build(root)\n  return {nodes: [...nodes], edges: [...edges]}\n}\n\n/*\nx = new Value(1.0);\ny = x.mul(2).add(1).relu();\ny.backward();\nreturn trace(y)\n*/","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":0,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{},"source":"This is the draw function. It makes use of a built in variable called `graph`. This variable is available on this website. It is not standard, so be aware that these notebooks primarily work with this site. "},{"cell_type":"code","execution_count":0,"metadata":{},"source":"const wait = async (delay) => {\n  return new Promise((resolve, reject) => setTimeout(resolve, delay));\n}\n\ndraw = async (root, labels = false, delay = 10) => {\n  const labelOptions = {\n    fontSize: '24px',\n    backgroundColor: \"transparent\"\n  }; \n  const { nodes, edges } = trace(root)\n  let vertexOptions;\n  \n  for(const n of nodes){\n    vertexOptions = {\n      id: `${n.id}`,\n      size: 0.25\n    }\n    if(labels){\n      vertexOptions.label = {\n        text: `{ data ${n.data.toFixed(2)} | grad ${n.grad.toFixed(2)} }`,\n        ...labelOptions\n      }\n    }\n\n    await wait(delay)\n    graph.addVertex(vertexOptions)\n    \n    if(n.op){\n      vertexOptions = {\n        id: `${n.id}${n.op}`,\n        size: 0.25\n      }\n      if(labels){\n        vertexOptions.label = {\n          text: vertexOptions.id,\n          ...labelOptions\n        }\n      }\n\n      await wait(delay);\n      graph.addVertex(vertexOptions);\n      graph.addEdge(\n        `${n.id}${n.op}`, \n        n.id,\n        {\n          arrow: true\n        }\n      );\n    }\n  }\n\n  for([n, m] of Object.values(edges)){\n    graph.addEdge(n.id, m.id, {arrow: true})\n  }\n}\n\n/*\nx = new Value(1.0);\ny = x.mul(new Value(2)).add(new Value(1)).relu();\ny.backward();\ndraw(y);\n*/\n\n/*\nn = new Neuron(2);\nx = [new Value(1.0), new Value(-2.0)]\ny = n.call(x)\ny.backward()\ndraw(y);\n*/","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":0,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":"Layer = class Layer {\n  constructor(nin, nout, nonlin){\n    this.neurons = Array(nout).fill(0).map(() => new Neuron(nin, nonlin));\n  }\n\n  call(x){\n    const outs = this.neurons.map(n => n.call(x));\n    return outs.length == 1 ? outs[0] : outs;\n  }\n\n  parameters(){\n    const params = [];\n    for(let neuron of this.neurons){\n      for(let p of neuron.parameters()){\n        params.push(p);\n      }\n    }\n    \n    return params;\n  }\n\n  toString(){\n    return `Layer of [{${this.neurons.map(n => n.toString()).join(', ')}}]`\n  }\n}\n\n/*\nconst l = new Layer(new Value(2), new Value(2));\nconst x = new Value(3);\nconst y = l.call(x);\ndraw(y)\n*/","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":0,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":"MLP = class MLP {\n  constructor(nin, nouts){\n    const sz = [nin].concat(...nouts);\n    this.layers = Array(nouts.length).fill(0).map((_, i) => new Layer(sz[i], sz[i+1], i < nouts.length - 1))\n  }\n\n  call(x){\n    for(let layer of this.layers){\n      x = layer.call(x);\n    }\n    \n    return x;\n  }\n\n  parameters(){\n    let params = [];\n    for(let layer of this.layers){\n      for(let p of layer.parameters()){\n        params.push(p);\n      }\n    }\n\n    return params;\n  }\n\n  toString(){\n    return `MLP of [{${this.layers.map(l => l.toString()).join(', ')}}]`\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":0,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{},"source":"Here is where it all comes together. We train a small MLP to imitate some function, calculate its loss, and visualize the gradient graph of this loss. "},{"cell_type":"code","execution_count":0,"metadata":{},"source":"let x = [2.0, 3.0, -1.0]\nlet mlp = new MLP(3, [4, 4, 1]);\nlet rounds = 1000;\nmlp.call(x)\n\n// inputs\nlet xs =[\n  [2.0, 3.0, -1.0],\n  [3.0, -1.0, 0.5],\n  [0.5, 1.0, 1.0],\n  [1.0, 1.0, -1.0]\n];\n\n// correct (training) outputs\nlet ys = [1.0, -1.0, -1.0, 1.0];\n\nconsole.log(`target values:<br>`, JSON.stringify(ys));\n\n// prediction by the MLP\nlet predictions = xs.map((x) => mlp.call(x));\n\n// essentially beefed up difference between correct outputs and mlp outputs\nlet loss = ys\n  .map((ygt, i) => predictions[i].sub(ygt).pow(2))\n  .reduce((acc, l) => acc.add(l), new Value(0));\n\n// calculate the gradients of all expressions' nodes\nloss.backward()\n// draw(loss)\n\n\n// log values to output so we can see\nconsole.log(`<br>prediction before optimization:<br>`, JSON.stringify(predictions.map(y => parseFloat(y.data.toFixed(3))), null, 2));\nconsole.log(`<br>starting loss:<br> ${loss.data.toFixed(3)}`);\n\n// optimization\n// adjust the mlp's parameters in the direction (up or down) multiplied by\n// some small constant to control the size of the variation steps.\nfor(var i=0; i<1000; i++){\n  // mlp predict for each input\n  predictions = xs.map((x) => mlp.call(x));\n  loss = ys\n    .map((ygt, i) => predictions[i].sub(ygt).pow(2))\n    .reduce((acc, l) => acc.add(l), new Value(0));\n\n  // reset the gradients (not sure why, but it was explained)\n  let params = mlp.parameters();\n  for(let p of params){\n    p.grad = 0.0;\n  }\n\n  // calculate the mlp's .grads\n  loss.backward();\n\n  // set value of p to -0.01 * p.grad.\n  // - - because we want the value to change in the direction of less wrong\n  // 0.01 some small step size, chosen to neither over-step, nor take too long.\n  // p.grad is an arrow \n  params = mlp.parameters();\n  for(let p of params){\n    p.data += -0.01 * p.grad;\n  }\n\n  // console.info(`${i} ${loss.data}`)\n}\n\nconsole.log(`<br>rounds of optimization:<br>${rounds}`)\nconsole.log(`<br>predictions after optimization:<br>`, JSON.stringify(predictions.map(y => parseFloat(y.data.toFixed(3))), null, 2));\nconsole.log(`<br>final loss:<br> ${loss.data.toFixed(3)}`);\n\ndraw(loss)","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":0,"metadata":{},"output_type":"execute_result"}]}]}